/**
 * â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 * â•‘                         LIVE VOICE CONVERSATION PAGE                       â•‘
 * â•‘                    Real-time Audio Streaming with Gemini                   â•‘
 * â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

import { useState, useRef, useCallback, useEffect } from "react";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { ScrollArea } from "@/components/ui/scroll-area";
import { Badge } from "@/components/ui/badge";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { Switch } from "@/components/ui/switch";
import { Label } from "@/components/ui/label";
import { Slider } from "@/components/ui/slider";
import {
  Mic,
  MicOff,
  Phone,
  PhoneOff,
  Volume2,
  VolumeX,
  ArrowLeft,
  Loader2,
  AlertCircle,
  Radio,
  Waves,
  Settings2,
  Zap,
} from "lucide-react";
import { Link } from "wouter";
import { cn } from "@/lib/utils";
import { useVoiceActivityDetection } from "@/hooks/use-voice-activity-detection";
import { useSpeechRecognition } from "@/hooks/use-speech-recognition";

interface TranscriptEntry {
  id: string;
  speaker: "user" | "ai";
  text: string;
  timestamp: Date;
}

type ConnectionState = "disconnected" | "connecting" | "connected" | "error";

const VOICES = [
  { value: "Kore", label: "Kore - Clear Female" },
  { value: "Puck", label: "Puck - Warm Male" },
  { value: "Charon", label: "Charon - Deep Male" },
  { value: "Fenrir", label: "Fenrir - Strong Male" },
  { value: "Aoede", label: "Aoede - Melodic Female" },
  { value: "Leda", label: "Leda - Soft Female" },
  { value: "Orus", label: "Orus - Authoritative Male" },
  { value: "Zephyr", label: "Zephyr - Gentle Neutral" },
];

export default function LivePage() {
  const [connectionState, setConnectionState] = useState<ConnectionState>("disconnected");
  const [isListening, setIsListening] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcript, setTranscript] = useState<TranscriptEntry[]>([]);
  const [interimText, setInterimText] = useState("");
  const [selectedVoice, setSelectedVoice] = useState("Kore");
  const [error, setError] = useState<string | null>(null);
  const [showSettings, setShowSettings] = useState(false);
  const [continuousMode, setContinuousMode] = useState(true);
  const [vadSensitivity, setVadSensitivity] = useState(0.015);
  const [userInterimTranscript, setUserInterimTranscript] = useState("");
  const [enableSTT, setEnableSTT] = useState(true);

  const sessionIdRef = useRef<string | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const playbackContextRef = useRef<AudioContext | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const scrollRef = useRef<HTMLDivElement>(null);
  const speakingTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const audioQueueRef = useRef<AudioBufferSourceNode[]>([]);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const userSpeechBufferRef = useRef<string[]>([]);
  const speechRecognitionRef = useRef<any>(null);

  const addTranscriptEntry = useCallback((speaker: "user" | "ai", text: string) => {
    const entry: TranscriptEntry = {
      id: crypto.randomUUID(),
      speaker,
      text,
      timestamp: new Date(),
    };
    setTranscript(prev => [...prev, entry]);
  }, []);

  const handleBargeIn = useCallback(() => {
    audioQueueRef.current.forEach(source => {
      try { source.stop(); } catch {}
    });
    audioQueueRef.current = [];
    
    if (speakingTimeoutRef.current) {
      clearTimeout(speakingTimeoutRef.current);
      speakingTimeoutRef.current = null;
    }
    
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({ type: "interrupt" }));
    }
    setIsSpeaking(false);
  }, []);

  // Speech recognition for interim transcripts and cognitive endpointing
  const speechRecognition = useSpeechRecognition(
    useCallback((result: { isFinal: boolean; transcript: string }) => {
      if (result.isFinal) {
        // Send final transcript through text channel for faster response initiation
        if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN && result.transcript.trim()) {
          wsRef.current.send(JSON.stringify({
            type: "text",
            text: result.transcript,
          }));
        }
        addTranscriptEntry("user", result.transcript);
        setUserInterimTranscript("");
      } else {
        // Show interim transcript
        setUserInterimTranscript(result.transcript);
      }
    }, [addTranscriptEntry]),
    useCallback(() => {
      setUserInterimTranscript("");
    }, []),
    { continuous: true, interimResults: true }
  );

  // Store speech recognition in ref for VAD callbacks
  useEffect(() => {
    speechRecognitionRef.current = speechRecognition;
  }, [speechRecognition]);

  // Voice Activity Detection for continuous listening mode
  const vadCallbacks = {
    onSpeechStart: useCallback(() => {
      console.log("[Live] User started speaking");
      if (continuousMode && isSpeaking) {
        // User is interrupting AI - trigger barge-in
        handleBargeIn();
      }
      userSpeechBufferRef.current = [];
      setUserInterimTranscript("");
      
      // Start speech recognition for transcription
      if (continuousMode && enableSTT && speechRecognitionRef.current && !speechRecognitionRef.current.isListening) {
        speechRecognitionRef.current.start();
      }
    }, [continuousMode, isSpeaking, handleBargeIn]),
    
    onSpeechEnd: useCallback(() => {
      console.log("[Live] User stopped speaking");
      
      // Stop speech recognition
      if (speechRecognitionRef.current && speechRecognitionRef.current.isListening) {
        speechRecognitionRef.current.stop();
      }
      
      // Clear interim transcript
      setUserInterimTranscript("");
    }, []),
    
    onVolumeChange: useCallback((volume: number) => {
      // Could be used for visual feedback in the UI
    }, []),
  };

  const vad = useVoiceActivityDetection(
    vadCallbacks.onSpeechStart,
    vadCallbacks.onSpeechEnd,
    vadCallbacks.onVolumeChange,
    { threshold: vadSensitivity, silenceDuration: 800, speechDuration: 200 }
  );

  const connect = useCallback(async () => {
    setConnectionState("connecting");
    setError(null);

    try {
      const sessionId = crypto.randomUUID();
      sessionIdRef.current = sessionId;

      const response = await fetch("/api/live/session", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          sessionId,
          voiceName: selectedVoice,
        }),
      });

      const data = await response.json();

      if (!data.success) {
        throw new Error(data.error || "Failed to create session");
      }

      const wsProtocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const ws = new WebSocket(`${wsProtocol}//${window.location.host}/api/live/stream/${sessionId}`);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log("[Live] WebSocket connected");
        setConnectionState("connected");
        
        // Auto-start listening in continuous mode
        if (continuousMode) {
          setTimeout(() => {
            startListening().catch(err => {
              console.error("[Live] Failed to auto-start listening:", err);
            });
          }, 100);
        }
      };

      ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          
          if (message.type === "audio") {
            playAudioChunk(message.data);
            setIsSpeaking(true);
          } else if (message.type === "transcript") {
            addTranscriptEntry("ai", message.text);
            setIsSpeaking(false);
          } else if (message.type === "text") {
            setInterimText(message.text);
          } else if (message.type === "end") {
            setIsSpeaking(false);
          } else if (message.type === "error") {
            setError(message.error);
          }
        } catch (err) {
          console.error("[Live] Failed to parse message:", err);
        }
      };

      ws.onerror = (err) => {
        console.error("[Live] WebSocket error:", err);
        setError("Connection error");
        setConnectionState("error");
      };

      ws.onclose = () => {
        console.log("[Live] WebSocket closed");
        if (connectionState === "connected") {
          setConnectionState("disconnected");
        }
      };

    } catch (err) {
      console.error("[Live] Failed to connect:", err);
      setError(err instanceof Error ? err.message : "Failed to connect");
      setConnectionState("error");
    }
  }, [selectedVoice, addTranscriptEntry, connectionState, continuousMode]);

  const disconnect = useCallback(async () => {
    stopListening();
    
    if (speakingTimeoutRef.current) {
      clearTimeout(speakingTimeoutRef.current);
      speakingTimeoutRef.current = null;
    }
    
    audioQueueRef.current.forEach(source => {
      try { source.stop(); } catch {}
    });
    audioQueueRef.current = [];
    
    if (playbackContextRef.current) {
      try { playbackContextRef.current.close(); } catch {}
      playbackContextRef.current = null;
    }
    
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    if (sessionIdRef.current) {
      try {
        await fetch(`/api/live/session/${sessionIdRef.current}`, {
          method: "DELETE",
        });
      } catch (err) {
        console.error("[Live] Failed to close session:", err);
      }
      sessionIdRef.current = null;
    }

    setConnectionState("disconnected");
    setIsListening(false);
    setIsSpeaking(false);
    setInterimText("");
  }, []);

  const startListening = useCallback(async () => {
    if (connectionState !== "connected") return;

    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      streamRef.current = stream;

      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      
      // Load the AudioWorklet processor
      try {
        await audioContextRef.current.audioWorklet.addModule("/audio-processor.js");
      } catch (e) {
        console.error("Failed to load audio processor worklet:", e);
        throw new Error("Failed to initialize audio processor");
      }

      const source = audioContextRef.current.createMediaStreamSource(stream);
      const workletNode = new AudioWorkletNode(audioContextRef.current, "audio-processor");
      workletNodeRef.current = workletNode;

      workletNode.port.onmessage = (event) => {
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;

        // In continuous mode, only send audio when VAD detects speech
        // In manual mode, always send audio
        if (!continuousMode || vad.isSpeaking) {
          // event.data is the ArrayBuffer containing Int16 PCM data
          const pcmBuffer = event.data;
          const uint8Array = new Uint8Array(pcmBuffer);
          
          // Convert to base64 for transmission
          let binaryString = "";
          const len = uint8Array.byteLength;
          for (let i = 0; i < len; i++) {
            binaryString += String.fromCharCode(uint8Array[i]);
          }
          const base64 = btoa(binaryString);
          
          wsRef.current.send(JSON.stringify({
            type: "audio",
            data: base64,
            mimeType: "audio/pcm",
          }));
        }
      };

      source.connect(workletNode);
      workletNode.connect(audioContextRef.current.destination);

      // Start VAD if in continuous mode
      if (continuousMode) {
        try {
          await vad.start();
        } catch (e) {
          console.error("[Live] Failed to start VAD:", e);
        }
      }

      setIsListening(true);
    } catch (err) {
      console.error("[Live] Failed to start listening:", err);
      setError("Microphone access denied");
    }
  }, [connectionState, continuousMode, vad]);

  const stopListening = useCallback(() => {
    // Stop VAD
    vad.stop();

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    workletNodeRef.current = null;
    setIsListening(false);
  }, [vad]);

  const playAudioChunk = useCallback((base64Data: string) => {
    if (isMuted) return;

    try {
      if (!playbackContextRef.current || playbackContextRef.current.state === "closed") {
        playbackContextRef.current = new AudioContext({ sampleRate: 24000 });
      }
      
      const audioContext = playbackContextRef.current;

      const binaryString = atob(base64Data);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }

      const audioBuffer = audioContext.createBuffer(1, bytes.length / 2, 24000);
      const channelData = audioBuffer.getChannelData(0);

      const int16Array = new Int16Array(bytes.buffer);
      for (let i = 0; i < int16Array.length; i++) {
        channelData[i] = int16Array[i] / 32768;
      }

      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      source.start();
      
      source.onended = () => {
        const index = audioQueueRef.current.indexOf(source);
        if (index > -1) {
          audioQueueRef.current.splice(index, 1);
        }
      };
      audioQueueRef.current.push(source);

      if (speakingTimeoutRef.current) {
        clearTimeout(speakingTimeoutRef.current);
      }
      speakingTimeoutRef.current = setTimeout(() => {
        setIsSpeaking(false);
      }, 500);
    } catch (err) {
      console.error("[Live] Failed to play audio:", err);
    }
  }, [isMuted]);

  useEffect(() => {
    if (scrollRef.current) {
      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;
    }
  }, [transcript]);

  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return (
    <div className="min-h-screen bg-background flex flex-col">
      <header className="border-b bg-card px-4 py-3 flex-shrink-0">
        <div className="container mx-auto flex items-center justify-between">
          <div className="flex items-center gap-3">
            <Link href="/">
              <Button variant="ghost" size="icon" data-testid="button-back">
                <ArrowLeft className="h-5 w-5" />
              </Button>
            </Link>
            <div className="flex-1">
              <div className="flex items-center gap-2">
                <Radio className="h-5 w-5 text-primary" />
                <h1 className="text-lg font-semibold">Live Voice</h1>
              </div>
              <p className="text-sm text-muted-foreground">Real-time AI voice conversation</p>
            </div>
          </div>

          <div className="flex items-center gap-2">
            <Badge
              variant={
                connectionState === "connected"
                  ? "default"
                  : connectionState === "connecting"
                  ? "secondary"
                  : connectionState === "error"
                  ? "destructive"
                  : "outline"
              }
              data-testid="badge-connection-state"
            >
              {connectionState === "connected" && (
                <span className="w-2 h-2 bg-green-500 rounded-full mr-1.5 animate-pulse" />
              )}
              {connectionState}
            </Badge>
            <Button
              variant="ghost"
              size="icon"
              onClick={() => setShowSettings(!showSettings)}
              data-testid="button-settings"
            >
              <Settings2 className="h-5 w-5" />
            </Button>
            <Button
              variant="ghost"
              size="icon"
              onClick={() => setIsMuted(!isMuted)}
              data-testid="button-mute"
            >
              {isMuted ? <VolumeX className="h-5 w-5" /> : <Volume2 className="h-5 w-5" />}
            </Button>
          </div>
        </div>
      </header>

      <main className="flex-1 container mx-auto px-4 py-6 flex flex-col gap-4 max-w-3xl overflow-hidden">
        {showSettings && connectionState === "disconnected" && (
          <div className="border border-border rounded-lg bg-muted/20 p-6 animate-in fade-in slide-in-from-top-2">
            <h3 className="text-base font-semibold mb-4">Voice Settings</h3>
            <div className="space-y-4">
                <div>
                  <Label htmlFor="voice-select" className="text-sm font-medium mb-1.5 block">AI Voice</Label>
                  <Select value={selectedVoice} onValueChange={setSelectedVoice}>
                    <SelectTrigger id="voice-select" data-testid="select-voice">
                      <SelectValue />
                    </SelectTrigger>
                    <SelectContent>
                      {VOICES.map((voice) => (
                        <SelectItem key={voice.value} value={voice.value}>
                          {voice.label}
                        </SelectItem>
                      ))}
                    </SelectContent>
                  </Select>
                </div>

                <div className="space-y-2">
                  <div className="flex items-center justify-between">
                    <Label htmlFor="continuous-mode" className="text-sm font-medium flex items-center gap-2">
                      <Zap className="h-4 w-4" />
                      Continuous Listening
                    </Label>
                    <Switch
                      id="continuous-mode"
                      checked={continuousMode}
                      onCheckedChange={setContinuousMode}
                      data-testid="switch-continuous-mode"
                    />
                  </div>
                  <p className="text-xs text-muted-foreground">
                    {continuousMode 
                      ? "Automatically detects when you speak and responds naturally"
                      : "Press the mic button each time you want to speak"}
                  </p>
                </div>

                {continuousMode && (
                  <div className="space-y-2">
                    <Label htmlFor="vad-sensitivity" className="text-sm font-medium">
                      Voice Detection Sensitivity: {vadSensitivity.toFixed(3)}
                    </Label>
                    <Slider
                      id="vad-sensitivity"
                      min={0.005}
                      max={0.05}
                      step={0.001}
                      value={[vadSensitivity]}
                      onValueChange={([value]) => {
                        setVadSensitivity(value);
                        vad.updateConfig({ threshold: value });
                      }}
                      data-testid="slider-vad-sensitivity"
                    />
                    <p className="text-xs text-muted-foreground">
                      Lower = more sensitive (picks up quieter speech)
                    </p>
                  </div>
                )}

                {continuousMode && (
                  <div className="space-y-2">
                    <div className="flex items-center justify-between">
                      <Label htmlFor="enable-stt" className="text-sm font-medium">
                        Speech-to-Text (Cognitive Endpointing)
                      </Label>
                      <Switch
                        id="enable-stt"
                        checked={enableSTT}
                        onCheckedChange={setEnableSTT}
                        data-testid="switch-enable-stt"
                      />
                    </div>
                    <p className="text-xs text-muted-foreground">
                      {enableSTT 
                        ? "Transcribes speech for faster AI response and interim feedback"
                        : "Uses audio-only processing (slightly slower)"}
                    </p>
                    {!speechRecognition.isSupported && (
                      <p className="text-xs text-amber-600">
                        âš ï¸ Speech recognition not supported in this browser
                      </p>
                    )}
                  </div>
                )}
              </div>
          </div>
        )}

        {error && (
          <div className="border border-destructive/30 rounded-lg bg-destructive/5 p-4 flex items-center gap-2 text-destructive">
            <AlertCircle className="h-4 w-4" />
            <span className="text-sm">{error}</span>
            <Button
              variant="ghost"
              size="sm"
              className="ml-auto"
              onClick={() => setError(null)}
            >
              Dismiss
            </Button>
          </div>
        )}

        <div className="border border-border rounded-lg bg-muted/20 flex flex-col min-h-[400px] flex-1">
          <div className="p-4 border-b border-border">
            <h3 className="text-base font-semibold flex items-center gap-2">
              <Waves className="h-4 w-4" />
              Conversation
            </h3>
            <p className="text-sm text-muted-foreground mt-1">
              {connectionState === "disconnected"
                ? "Connect to start a voice conversation"
                : connectionState === "connected"
                ? continuousMode
                  ? vad.isSpeaking
                    ? "ğŸ¤ Listening to you..."
                    : isListening
                    ? "ğŸ‘‚ Waiting for you to speak..."
                    : "Ready"
                  : isListening
                  ? "Listening..."
                  : "Press the mic button to speak"
                : ""}
            </p>
          </div>
          <div className="flex-1 flex flex-col p-4">
            <ScrollArea className="flex-1" ref={scrollRef}>
              {transcript.length === 0 ? (
                <div className="h-full flex items-center justify-center text-muted-foreground text-center p-8">
                  <div>
                    <Radio className="h-12 w-12 mx-auto mb-4 opacity-20" />
                    <p>Your conversation will appear here</p>
                    <p className="text-sm mt-1">Connect and start speaking to begin</p>
                  </div>
                </div>
              ) : (
                <div className="space-y-4">
                  {transcript.map((entry) => (
                    <div
                      key={entry.id}
                      className={cn(
                        "flex gap-3",
                        entry.speaker === "user" ? "justify-end" : "justify-start"
                      )}
                    >
                      <div
                        className={cn(
                          "max-w-[80%] rounded-2xl px-4 py-2",
                          entry.speaker === "user"
                            ? "bg-primary text-primary-foreground"
                            : "bg-muted"
                        )}
                        data-testid={`transcript-${entry.speaker}-${entry.id}`}
                      >
                        <p className="text-sm">{entry.text}</p>
                        <span className="text-xs opacity-60 mt-1 block">
                          {entry.timestamp.toLocaleTimeString()}
                        </span>
                      </div>
                    </div>
                  ))}
                  {userInterimTranscript && (
                    <div className="flex gap-3 justify-end">
                      <div className="max-w-[80%] rounded-2xl px-4 py-2 bg-primary/20 border border-primary/50 border-dashed">
                        <p className="text-sm text-foreground italic">{userInterimTranscript}</p>
                        <span className="text-xs opacity-60 mt-1 block">Speaking...</span>
                      </div>
                    </div>
                  )}
                  {interimText && (
                    <div className="flex gap-3 justify-start">
                      <div className="max-w-[80%] rounded-2xl px-4 py-2 bg-muted/50 border border-dashed">
                        <p className="text-sm text-muted-foreground italic">{interimText}</p>
                      </div>
                    </div>
                  )}
                  {isSpeaking && (
                    <div className="flex gap-3 justify-start">
                      <div className="rounded-2xl px-4 py-2 bg-muted flex items-center gap-2">
                        <div className="flex gap-1">
                          <span className="w-2 h-2 bg-primary rounded-full animate-bounce" style={{ animationDelay: "0ms" }} />
                          <span className="w-2 h-2 bg-primary rounded-full animate-bounce" style={{ animationDelay: "150ms" }} />
                          <span className="w-2 h-2 bg-primary rounded-full animate-bounce" style={{ animationDelay: "300ms" }} />
                        </div>
                        <span className="text-sm text-muted-foreground">Speaking...</span>
                        {continuousMode && (
                          <Button
                            variant="ghost"
                            size="sm"
                            onClick={handleBargeIn}
                            className="h-6 px-2 text-xs"
                            data-testid="button-barge-in"
                          >
                            Interrupt
                          </Button>
                        )}
                      </div>
                    </div>
                  )}
                </div>
              )}
            </ScrollArea>
          </div>
        </div>

        <div className="flex items-center justify-center gap-4 py-4">
          {connectionState === "disconnected" || connectionState === "error" ? (
            <Button
              size="lg"
              className="rounded-full h-16 w-16"
              onClick={connect}
              data-testid="button-connect"
            >
              <Phone className="h-6 w-6" />
            </Button>
          ) : connectionState === "connecting" ? (
            <Button
              size="lg"
              className="rounded-full h-16 w-16"
              disabled
            >
              <Loader2 className="h-6 w-6 animate-spin" />
            </Button>
          ) : (
            <>
              {!continuousMode && (
                <Button
                  size="lg"
                  variant={isListening ? "default" : "outline"}
                  className={cn(
                    "rounded-full h-16 w-16 transition-all",
                    isListening && "ring-4 ring-primary/30 animate-pulse",
                    vad.isSpeaking && "ring-4 ring-green-500/30"
                  )}
                  onClick={isListening ? stopListening : startListening}
                  data-testid="button-mic"
                >
                  {isListening ? (
                    <Mic className="h-6 w-6" />
                  ) : (
                    <MicOff className="h-6 w-6" />
                  )}
                </Button>
              )}
              <Button
                size="lg"
                variant="destructive"
                className="rounded-full h-16 w-16"
                onClick={disconnect}
                data-testid="button-disconnect"
              >
                <PhoneOff className="h-6 w-6" />
              </Button>
            </>
          )}
        </div>

        <p className="text-center text-xs text-muted-foreground">
          {connectionState === "connected" && continuousMode
            ? "Just speak naturally. The AI will listen and respond automatically."
            : connectionState === "connected" && isListening
            ? "Speak naturally. The AI can hear you in real-time."
            : connectionState === "connected"
            ? "Tap the microphone to start speaking"
            : "Tap the phone button to connect"}
        </p>
      </main>
    </div>
  );
}
